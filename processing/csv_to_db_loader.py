"""Load CSV files into PostgreSQL database.

This module provides functionality to load CSV files (generated by csv_exporter.py)
into a PostgreSQL database using bulk insert operations.

Usage:
    loader = CSVToDBLoader(conn_string=CS)
    loader.load_csv_directory('./csv_output', commit=True)
    loader.close()
"""

import csv
import os
import pathlib
from typing import Optional, Callable, List, Dict
import psycopg2
from psycopg2.extras import execute_values


class CSVToDBLoader:
    """Loader for importing CSV files into PostgreSQL database.

    Contract:
      - Input: Directory containing CSV files from csv_exporter.py
      - Output: Counts of rows inserted per table
      - Error: Raises on failure, rolls back if we own the transaction
      - Progress: Optional callback for progress updates
    """

    def __init__(
        self,
        conn_string: Optional[str] = None,
        conn=None,
    ):
        """Initialize the CSV loader.

        Args:
            conn_string: PostgreSQL connection string
            conn: Existing psycopg2 connection (alternative to conn_string)
        """
        if conn is not None:
            self.conn = conn
            self._owns_connection = False
        elif conn_string is not None:
            self.conn = psycopg2.connect(conn_string)
            self._owns_connection = True
        else:
            raise ValueError("Either conn or conn_string must be provided")

        self.cur = self.conn.cursor()
        self._manage_tx = self._owns_connection

        # Mapping of CSV filename to (table_name, columns, has_id_mapping)
        self._table_config = {
            "sources.csv": (
                "sources",
                [
                    "source_id",
                    "source_name",
                    "source_abbrev",
                    "scopus_source_id",
                    "issn_print",
                    "issn_electronic",
                    "publisher",
                    "source_type",
                ],
                True,
            ),
            "affiliations.csv": (
                "affiliations",
                [
                    "affiliation_id",
                    "scopus_affiliation_id",
                    "affiliation_name",
                    "city",
                    "state",
                    "country",
                    "postal_code",
                ],
                True,
            ),
            "authors.csv": (
                "authors",
                [
                    "author_id",
                    "auid",
                    "surname",
                    "given_name",
                    "initials",
                    "indexed_name",
                ],
                True,
            ),
            "subject_areas.csv": (
                "subject_areas",
                ["subject_area_id", "subject_code", "subject_name", "subject_abbrev"],
                True,
            ),
            "keywords.csv": (
                "keywords",
                ["keyword_id", "keyword", "keyword_type"],
                True,
            ),
            "papers.csv": (
                "papers",
                [
                    "paper_id",
                    "scopus_id",
                    "eid",
                    "doi",
                    "title",
                    "abstract",
                    "publication_date",
                    "publication_year",
                    "source_id",
                    "source_type",
                    "volume",
                    "issue",
                    "page_range",
                    "start_page",
                    "end_page",
                    "cited_by_count",
                    "open_access",
                    "document_type",
                    "subtype_description",
                ],
                True,
            ),
            "paper_authors.csv": (
                "paper_authors",
                ["paper_author_id", "paper_id", "author_id", "author_sequence"],
                False,
            ),
            "paper_author_affiliations.csv": (
                "paper_author_affiliations",
                ["paper_author_id", "affiliation_id"],
                False,
            ),
            "paper_keywords.csv": (
                "paper_keywords",
                ["paper_id", "keyword_id", "keyword_type"],
                False,
            ),
            "paper_subject_areas.csv": (
                "paper_subject_areas",
                ["paper_id", "subject_area_id"],
                False,
            ),
            "reference_papers.csv": (
                "reference_papers",
                [
                    "paper_id",
                    "reference_sequence",
                    "reference_fulltext",
                    "cited_year",
                    "cited_volume",
                    "cited_pages",
                ],
                False,
            ),
            "funding_agencies.csv": (
                "funding_agencies",
                [
                    "agency_id",
                    "agency_name",
                    "agency_acronym",
                    "agency_country",
                    "scopus_agency_id",
                ],
                True,
            ),
            "paper_funding.csv": (
                "paper_funding",
                ["paper_id", "agency_id", "grant_id"],
                False,
            ),
        }

        # Processing order (dimension tables first, then fact tables, then relationships)
        self._load_order = [
            "sources.csv",
            "affiliations.csv",
            "authors.csv",
            "subject_areas.csv",
            "keywords.csv",
            "funding_agencies.csv",
            "papers.csv",
            "paper_authors.csv",
            "paper_author_affiliations.csv",
            "paper_keywords.csv",
            "paper_subject_areas.csv",
            "reference_papers.csv",
            "paper_funding.csv",
        ]

    def load_csv_directory(
        self,
        csv_dir: str,
        commit: bool = True,
        progress_callback: Optional[Callable[[str, int], None]] = None,
    ) -> Dict[str, int]:
        """Load all CSV files from a directory into the database.

        Args:
            csv_dir: Directory containing CSV files
            commit: Whether to commit the transaction
            progress_callback: Optional callback(table_name, row_count)

        Returns:
            Dictionary mapping table names to row counts inserted
        """
        csv_path = pathlib.Path(csv_dir)
        if not csv_path.exists():
            raise ValueError(f"CSV directory does not exist: {csv_dir}")

        results = {}
        own_tx = self._manage_tx and commit

        try:
            for csv_file in self._load_order:
                file_path = csv_path / csv_file
                if not file_path.exists():
                    print(f"Warning: {csv_file} not found, skipping")
                    continue

                table_name, columns, has_id_mapping = self._table_config[csv_file]
                print(f"\nLoading {csv_file} into {table_name}...")

                row_count = self._load_csv_file(
                    file_path, table_name, columns, has_id_mapping
                )
                results[table_name] = row_count

                if progress_callback:
                    try:
                        progress_callback(table_name, row_count)
                    except Exception:
                        pass

                print(f"  ✓ Loaded {row_count:,} rows")

            if own_tx:
                self.conn.commit()
                print("\n✓ All data committed successfully")

            return results

        except Exception as e:
            if own_tx:
                try:
                    self.conn.rollback()
                    print("\n✗ Transaction rolled back due to error")
                except Exception:
                    pass
            raise

    def _load_csv_file(
        self,
        file_path: pathlib.Path,
        table_name: str,
        columns: List[str],
        has_id_mapping: bool,
    ) -> int:
        """Load a single CSV file into a database table.

        Args:
            file_path: Path to CSV file
            table_name: Name of target database table
            columns: List of column names
            has_id_mapping: Whether this table has an auto-increment ID that should be mapped

        Returns:
            Number of rows inserted
        """
        rows = []
        with open(file_path, "r", encoding="utf-8") as f:
            reader = csv.reader(f)
            header = next(reader)  # Skip header

            # Validate header matches expected columns
            if header != columns:
                raise ValueError(
                    f"CSV header mismatch for {file_path.name}. "
                    f"Expected: {columns}, Got: {header}"
                )

            for row in reader:
                # Convert empty strings to None for NULL values
                processed_row = []
                for val in row:
                    if val == "":
                        processed_row.append(None)
                    elif val.lower() == "true":
                        processed_row.append(True)
                    elif val.lower() == "false":
                        processed_row.append(False)
                    else:
                        processed_row.append(val)
                rows.append(tuple(processed_row))

        if not rows:
            return 0

        # Build INSERT statement based on table configuration
        if has_id_mapping:
            # For dimension tables with IDs, use ON CONFLICT DO NOTHING
            # and map the synthetic CSV IDs to database IDs
            if table_name in [
                "sources",
                "affiliations",
                "authors",
                "subject_areas",
                "keywords",
                "funding_agencies",
            ]:
                # These tables have natural keys we can use for conflict resolution
                conflict_columns = self._get_conflict_columns(table_name)
                placeholders = ",".join(["%s"] * len(columns))
                cols_str = ",".join(columns)

                if conflict_columns:
                    # Use natural key for upsert
                    sql = f"""
                        INSERT INTO {table_name} ({cols_str})
                        VALUES %s
                        ON CONFLICT ({conflict_columns}) DO NOTHING
                    """
                else:
                    sql = f"""
                        INSERT INTO {table_name} ({cols_str})
                        VALUES %s
                    """
            elif table_name == "papers":
                # Papers use scopus_id as natural key
                placeholders = ",".join(["%s"] * len(columns))
                cols_str = ",".join(columns)
                sql = f"""
                    INSERT INTO {table_name} ({cols_str})
                    VALUES %s
                    ON CONFLICT (scopus_id) DO UPDATE SET
                        cited_by_count = EXCLUDED.cited_by_count,
                        updated_at = CURRENT_TIMESTAMP
                """
            else:
                placeholders = ",".join(["%s"] * len(columns))
                cols_str = ",".join(columns)
                sql = f"INSERT INTO {table_name} ({cols_str}) VALUES %s"
        else:
            # For relationship tables without synthetic IDs
            placeholders = ",".join(["%s"] * len(columns))
            cols_str = ",".join(columns)

            # Get conflict resolution for relationship tables
            conflict_cols = self._get_relationship_conflict_columns(table_name)
            if conflict_cols:
                sql = f"""
                    INSERT INTO {table_name} ({cols_str})
                    VALUES %s
                    ON CONFLICT ({conflict_cols}) DO NOTHING
                """
            else:
                sql = f"INSERT INTO {table_name} ({cols_str}) VALUES %s"

        # Execute bulk insert
        execute_values(self.cur, sql, rows)

        return len(rows)

    def _get_conflict_columns(self, table_name: str) -> Optional[str]:
        """Get natural key columns for conflict resolution."""
        conflict_map = {
            "sources": "scopus_source_id",
            "affiliations": "scopus_affiliation_id",
            "authors": "auid",
            "subject_areas": "subject_code",
            "keywords": "keyword",
            "funding_agencies": "scopus_agency_id",
        }
        return conflict_map.get(table_name)

    def _get_relationship_conflict_columns(self, table_name: str) -> Optional[str]:
        """Get conflict columns for relationship tables."""
        conflict_map = {
            "paper_authors": "paper_id, author_id",
            "paper_author_affiliations": "paper_author_id, affiliation_id",
            "paper_keywords": "paper_id, keyword_id",
            "paper_subject_areas": "paper_id, subject_area_id",
            "reference_papers": "paper_id, reference_sequence",
            "paper_funding": None,  # No unique constraint
        }
        return conflict_map.get(table_name)

    def close(self):
        """Close database connection and cursor."""
        try:
            self.cur.close()
        except Exception:
            pass
        if self._owns_connection:
            try:
                self.conn.close()
            except Exception:
                pass


class AsyncCSVToDBLoader:
    """Async wrapper for CSVToDBLoader using thread pool."""

    def __init__(self, conn_string: str, max_workers: int = 4):
        """Initialize async CSV loader.

        Args:
            conn_string: PostgreSQL connection string
            max_workers: Number of worker threads
        """
        import concurrent.futures

        self.conn_string = conn_string
        self._executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers)

    async def load_csv_directory_async(
        self,
        csv_dir: str,
        commit: bool = True,
        progress_callback: Optional[Callable[[str, int], None]] = None,
    ) -> Dict[str, int]:
        """Async load CSV directory into database.

        Args:
            csv_dir: Directory containing CSV files
            commit: Whether to commit the transaction
            progress_callback: Optional callback(table_name, row_count)

        Returns:
            Dictionary mapping table names to row counts inserted
        """
        import asyncio

        loop = asyncio.get_running_loop()

        def _worker():
            loader = CSVToDBLoader(conn_string=self.conn_string)
            try:
                return loader.load_csv_directory(
                    csv_dir, commit=commit, progress_callback=progress_callback
                )
            finally:
                loader.close()

        return await loop.run_in_executor(self._executor, _worker)

    async def close(self):
        """Close the executor."""
        import asyncio

        loop = asyncio.get_running_loop()
        await loop.run_in_executor(None, self._executor.shutdown, True)
